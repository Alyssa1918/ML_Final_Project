{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alyssa1918/ML_Final_Project/blob/main/CPTS_437_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[IMDB Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/code)\n",
        "\n",
        "[Github](https://github.com/Alyssa1918/ML_Final_Project/tree/main)"
      ],
      "metadata": {
        "id": "fexbR-ao3hwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive to load in dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zxSSn80k_MCg",
        "outputId": "6d4672d8-1737-45de-9b6e-f311140bff6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# reading in the IMDB csv file\n",
        "file = ('/content/drive/MyDrive/IMDB Dataset.csv')\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# displaying the contents of the csv file\n",
        "print(df, type(df))"
      ],
      "metadata": {
        "id": "UBKNXyeu9AfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa12b19-8e8a-43a3-9122-914b4cb426f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  review sentiment\n",
            "0      One of the other reviewers has mentioned that ...  positive\n",
            "1      A wonderful little production. <br /><br />The...  positive\n",
            "2      I thought this was a wonderful way to spend ti...  positive\n",
            "3      Basically there's a family where a little boy ...  negative\n",
            "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "...                                                  ...       ...\n",
            "49995  I thought this movie did a down right good job...  positive\n",
            "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
            "49997  I am a Catholic taught in parochial elementary...  negative\n",
            "49998  I'm going to have to disagree with the previou...  negative\n",
            "49999  No one expects the Star Trek movies to be high...  negative\n",
            "\n",
            "[50000 rows x 2 columns] <class 'pandas.core.frame.DataFrame'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "-ciSGRN67Cqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "587bf680-94dd-45f5-abfa-95d3bbd16889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly select 10000 samples from positive and 10000 from negative to ensure balanced classes\n",
        "\n",
        "positive_samples = df[df['sentiment'] == 'positive'].sample(n=10000, random_state=42)\n",
        "negative_samples = df[df['sentiment'] == 'negative'].sample(n=10000, random_state=42)\n",
        "\n",
        "# Concatenate the randomly selected samples\n",
        "df = pd.concat([positive_samples, negative_samples])\n",
        "del positive_samples, negative_samples\n",
        "\n",
        "# Shuffle the DataFrame to ensure samples are in random order\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "rm2LZDseB9l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing, Transformer, Pooling\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def encode_and_pool(reviews):\n",
        "    token_ids = tokenizer(reviews, truncation=True, padding=\"max_length\", max_length=512, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**token_ids)\n",
        "    pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "    del token_ids, outputs\n",
        "    return pooled_output.squeeze().detach().numpy()\n",
        "\n",
        "batch_size = 100\n",
        "pooled_output_all = []\n",
        "\n",
        "for i in range(0, len(df['review']), batch_size):\n",
        "    print(f\"on sample {i+1}\")\n",
        "\n",
        "    # process batch data\n",
        "    pooled_output_all.extend(encode_and_pool(df['review'].iloc[i:i+batch_size].tolist()))\n",
        "\n",
        "    print(f\"pooled_output_all list length: {len(pooled_output_all)}\")\n",
        "\n",
        "# save dataframe to file on local machine\n",
        "df['pooled_output'] = pooled_output_all\n",
        "df.to_csv('pooled.csv')\n",
        "files.download('pooled.csv')"
      ],
      "metadata": {
        "id": "AEYXRvOvB_lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# load in pooled.csv\n",
        "file = ('/content/drive/MyDrive/pooled.csv')\n",
        "pooled_df = pd.read_csv(file)\n",
        "\n",
        "# convert pooled_output column to list of numbers (currently strings)\n",
        "# will take a few minutes to run\n",
        "def string_to_nums(list_of_strings):\n",
        "    numbers_as_strings = list_of_strings.strip('[]\\n').split()\n",
        "    return [ast.literal_eval(num) for num in numbers_as_strings]\n",
        "\n",
        "pooled_df['pooled_output'] = pooled_df['pooled_output'].apply(string_to_nums)\n",
        "\n",
        "# split the dataset into training and testing sets where the test set is 20% of the original dataset.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(pooled_df['pooled_output'].tolist(), pooled_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# convert Y labels to an numpy array. Recall that the X value are already converted to numpy arrays during encode_and_pool.\n",
        "Y_train = Y_train.values\n",
        "Y_test = Y_test.values"
      ],
      "metadata": {
        "id": "6iOrevj0akE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# hypertuning k for KNN\n",
        "\n",
        "# generate k values\n",
        "k_values = range(1, 300, 2)\n",
        "\n",
        "# set parameter to tune (k values)\n",
        "grid_params = { 'n_neighbors' : k_values}\n",
        "\n",
        "# create instance of gridsearch\n",
        "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n",
        "\n",
        "# fit the model on our train set\n",
        "g_res = gs.fit(X_train, Y_train)\n",
        "\n",
        "# find the best score\n",
        "g_res.best_score_\n",
        "\n",
        "# get the hyperparameters with the best score\n",
        "opt_k_value = g_res.best_params_['n_neighbors']\n",
        "print(opt_k_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIczRZn1uXgH",
        "outputId": "ce557daa-98cf-4d58-c4c4-f3e136329948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 150 candidates, totalling 450 fits\n",
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Classifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# create instance of KNN and train\n",
        "KNN = KNeighborsClassifier(n_neighbors = 17)\n",
        "KNN.fit(X_train, Y_train)\n",
        "\n",
        "# get predictions for KNN\n",
        "KNN_Y_test_pred = KNN.predict(X_test)"
      ],
      "metadata": {
        "id": "LW-KvwwvAWVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "# create instance of SVM and train\n",
        "SVM = svm.SVC()\n",
        "SVM.fit(X_train, Y_train)\n",
        "\n",
        "# get predictions for SVM\n",
        "SVM_Y_test_pred = SVM.predict(X_test)"
      ],
      "metadata": {
        "id": "awYUk0Fvvdrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perceptron Classifier\n",
        "\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# create instance of Perceptron and train\n",
        "perceptron = Perceptron()\n",
        "perceptron.fit(X_train, Y_train)\n",
        "\n",
        "# get predictions for Perceptron\n",
        "Perceptron_Y_test_pred = perceptron.predict(X_test)"
      ],
      "metadata": {
        "id": "MOfrSU5wv9fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# create instance of Perceptron and train\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, Y_train)\n",
        "\n",
        "# get predictions for Decision Tree\n",
        "DT_Y_test_pred = decision_tree.predict(X_test)"
      ],
      "metadata": {
        "id": "yKcYKthgwPjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate models' performance via f1 score and bootstrapping\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Bootstrapping and F1 measurement for 10 bootstrap iterations\n",
        "KNN_scores = []\n",
        "SVM_scores = []\n",
        "Perceptron_scores = []\n",
        "DT_scores = []\n",
        "\n",
        "for i in range(10):  # for each bootstrap iteration\n",
        "  boot_samples = np.random.randint(low = 0, high = len(Y_test)-1, size = 1000) # sample (with replacement) a set of indices of 1000 test samples\n",
        "\n",
        "  # get their corresponding prediction\n",
        "  KNN_Y_pred_boot = KNN_Y_test_pred[boot_samples]\n",
        "  SVM_Y_pred_boot = SVM_Y_test_pred[boot_samples]\n",
        "  Perceptron_Y_pred_boot = Perceptron_Y_test_pred[boot_samples]\n",
        "  DT_Y_pred_boot = DT_Y_test_pred[boot_samples]\n",
        "\n",
        "  # get their corresponding ground-truth value\n",
        "  Y_test_boot = Y_test[boot_samples]\n",
        "\n",
        "  # evaluate the F1 measurement & store\n",
        "  KNN_scores.append(f1_score(Y_test_boot, KNN_Y_pred_boot, pos_label='positive'))\n",
        "  SVM_scores.append(f1_score(Y_test_boot, SVM_Y_pred_boot, pos_label='positive'))\n",
        "  Perceptron_scores.append(f1_score(Y_test_boot, Perceptron_Y_pred_boot, pos_label='positive'))\n",
        "  DT_scores.append(f1_score(Y_test_boot, DT_Y_pred_boot, pos_label='positive'))\n",
        "\n",
        "KNN_scores = np.array(KNN_scores)\n",
        "SVM_scores = np.array(SVM_scores)\n",
        "Perceptron_scores = np.array(Perceptron_scores)\n",
        "DT_scores = np.array(DT_scores)\n",
        "\n",
        "print(f\"Average F1 Score for KNN: {KNN_scores.mean()}\")\n",
        "print(f\"Average F1 Score for SVM: {SVM_scores.mean()}\")\n",
        "print(f\"Average F1 Score for Perceptron: {Perceptron_scores.mean()}\")\n",
        "print(f\"Average F1 Score for Decision Tree: {DT_scores.mean()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDDs5i3Iwr1o",
        "outputId": "1afa2c58-0498-4500-9b5c-5fc0d28d3740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average F1 Score for KNN: 0.7611507903070905\n",
            "Average F1 Score for SVM: 0.8733311273376934\n",
            "Average F1 Score for Perceptron: 0.7376342464208909\n",
            "Average F1 Score for Decision Tree: 0.7020079642148629\n"
          ]
        }
      ]
    }
  ]
}