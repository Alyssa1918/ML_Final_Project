{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alyssa1918/ML_Final_Project/blob/main/CPTS_437_Final_Project_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Code Demo</h1>"
      ],
      "metadata": {
        "id": "Tld8Rl7KxaPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Part One: How to download/install the code</h2>"
      ],
      "metadata": {
        "id": "Svpida4GxY9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two options to download/install our code. Option 1 is from the google-colab notebook directly and option two is from our Github.\n",
        "\n",
        "<h3>Option 1</h3>\n",
        "\n",
        "In order to download/install our code from google colab into your google drive, you will need to go to this link: https://colab.research.google.com/drive/1D7EUnZtDOcqwN8pMxG3NiV_pccAtCmF3?usp=sharing. It will take you to our google-colab that has all of the code for our project.\n",
        "\n",
        "Once there, you can go to \"File\" and click on \"Save a copy in Drive\" to save a copy of our code to your local google drive. Once you have a copy of our code on your drive, you will be able to go into the google-colab file and edit and run the program.\n",
        "\n",
        "<h3>Option 2</h3>\n",
        "\n",
        "If you would prefer to save a copy of our code via GitHub, you will need to our GitHub page located here: https://github.com/Alyssa1918/ML_Final_Project/tree/main.\n",
        "\n",
        "Once there, click on the green \"Code\" button and then click on \"Download ZIP\". This will download our project to your local computer's downloads file as a compressed zip file. From your downloads file, you can extract the project files from the compressed zip file by double clicking the compressed zip file and then clicking on \"Extract All\". Which will prompt you to give it a location to the extract the code to.\n",
        "\n",
        "Once the code is extracted, you can open the CPTS_437_Final_Project.ipynb in google-colab by going to your google drive. Clicking on \"New\", then in the following drop down, \"more\" and then \"Google Colaboratory\". Which will create a new google-colab project in your google drive.\n",
        "\n",
        "From there, go into your new google-colab project and click on \"File\" and then \"Open notebook\". This will create a pop-up in the middle of your screen with which you can select \"Upload\" and then \"Browse\". Which will open your local computer's file system and allow you to naviagte to wherever you extracted the CPTS_437_Final_Project.ipynb file to. Once you have navigated to the CPTS_437_Final_Project.ipynb file, click on it and press \"Open\". Which will open the file in google-colab where you can edit and run it.\n",
        "\n"
      ],
      "metadata": {
        "id": "WOZkvf3jxzV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Part Two: How to run the code</h2>"
      ],
      "metadata": {
        "id": "OUi4CxehxpqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have downloaded the code, open the code in google-colab by selecting the project file from your google drive. In order to run the code, you will have two options; either you can run all of the code including the encoder and pooling as in option 1, or you can load in use our saved pre-encoded and pooled information file and just run the code for the KNN, SVM, Decision Tree, and Perceptron algorithms as in option 2.\n",
        "\n",
        "It is recommened to use option 2 to quickly recreate our results since encoding and pooling the part of the IMDB dataset we used will take around 7 and a half hours.\n",
        "\n"
      ],
      "metadata": {
        "id": "69wJaJmy5oSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 1</h3>\n",
        "\n",
        "First, we will need to download the IMDb dataset to your google drive. Go to the following link https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/code. It will take you to IMDB dataset as hosted by Kaggle. Click on \"Download\" and it will be downloaded to your local computer's downloads file.\n",
        "\n",
        "Next, naviagte back to your google drive and click on \"New\" and then \"File upload\". Which will open a pop-up of your local computer drive where you can naviagte to your downloads file, click on \"IMDB Dataset.csv\", and then click \"Open\". Which will upload the \"IMDB Dataset.csv\" to your google drive.\n",
        "\n",
        "Navigate to your google drive and open up your copy of the project. Go ahead and run the first block of code on your copy of the project which will look like the following code block.\n"
      ],
      "metadata": {
        "id": "5BJJUyKjsbm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive to load in dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i0BnaRMh_s1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will open a pop-up that will ask you to connect your local google drive to the project. Click on \"Connect to Google Drive\" and then use the following pop-up to log into your google drive account. Once logged into your account, press \"Allow\" and your google drive will be synced to the project.\n",
        "\n",
        "Find the second block of code on your copy of the project. It will look like the following code block."
      ],
      "metadata": {
        "id": "y_h1LGcb_ti3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# reading in the IMDB csv file\n",
        "file = ('/content/drive/MyDrive/IMDB Dataset.csv')\n",
        "df = pd.read_csv(file)\n",
        "\n",
        "# displaying the contents of the csv file\n",
        "print(df, type(df))"
      ],
      "metadata": {
        "id": "M5SrP2FyBp5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once there, click on the \"Files\" button on the left-most side of your screen. Then click on \"drive\" and then \"MyDrive\". Find the \"IMDB Dataset.csv\" within your google-drive files and right click it. Then press on \"Copy Path\". Find the line that reads \"file = ('/content/drive/MyDrive/IMDB Dataset.csv')\" within the code block. Delete the part that reads /content/drive/MyDrive/IMBD Dataset.csv and copy and paste your \"IMDB Dataset.csv\" pathway into its location.\n",
        "\n",
        "Go ahead and run the second block of code on your copy of the project after you have done this to load in the IMDB dataset using pandas.read_csv(). Afterwards, run the third code block to pip install the transformers library from huggingface.\n",
        "\n",
        "Next, run the fourth code block on your copy of the project. It will look like the following code block."
      ],
      "metadata": {
        "id": "bEPiYv0sBtzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly select 10000 samples from positive and 10000 from negative to ensure balanced classes\n",
        "\n",
        "positive_samples = df[df['sentiment'] == 'positive'].sample(n=10000, random_state=42)\n",
        "negative_samples = df[df['sentiment'] == 'negative'].sample(n=10000, random_state=42)\n",
        "\n",
        "# Concatenate the randomly selected samples\n",
        "df = pd.concat([positive_samples, negative_samples])\n",
        "del positive_samples, negative_samples\n",
        "\n",
        "# Shuffle the DataFrame to ensure samples are in random order\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "VzTQ30Q9DB5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the fourth code block will randomly select 10,000 positive and negitive samples from the 50,000 available samples in the IMDB dataframe. They will then be stored together in one datatframe and shuffled to ensure the samples are in random order.\n",
        "\n",
        "Go ahead and run the fifth code block on your copy of the project. It will look like the following code block."
      ],
      "metadata": {
        "id": "gfJuiEWMDFLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing, Transformer, Pooling\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# Load pre-trained DistilBERT model and tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def encode_and_pool(reviews):\n",
        "    token_ids = tokenizer(reviews, truncation=True, padding=\"max_length\", max_length=512, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**token_ids)\n",
        "    pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "    del token_ids, outputs\n",
        "    return pooled_output.squeeze().detach().numpy()\n",
        "\n",
        "batch_size = 100\n",
        "pooled_output_all = []\n",
        "\n",
        "for i in range(0, len(df['review']), batch_size):\n",
        "    print(f\"on sample {i+1}\")\n",
        "\n",
        "    # process batch data\n",
        "    pooled_output_all.extend(encode_and_pool(df['review'].iloc[i:i+batch_size].tolist()))\n",
        "\n",
        "    print(f\"pooled_output_all list length: {len(pooled_output_all)}\")\n",
        "\n",
        "# save dataframe to file on local machine\n",
        "df['pooled_output'] = pooled_output_all\n",
        "df.to_csv('pooled.csv')\n",
        "files.download('pooled.csv')"
      ],
      "metadata": {
        "id": "nvfEwa2UIoV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the fifth block of code on your copy of the project will use the DistilBert tokenizer and model to encode the 20,000 samples in the dataframe. Pooling the resulting vectors down into one numpy array of 768 elements for each of the 20,000 samples. This will then be saved to your local computer's downloads file as pooled.csv.\n",
        "\n",
        "Note that running the fifth block of code will take about 7 and a half hours. If you do not wish to wait the 7 and a half hours of run time, feel free to see option 2 and use our pooled.csv.  \n",
        "\n",
        "Now that the fifth code block is finally done running, you will need to download your pooled.csv to your google drive. In order to do this you will need to naviagte back to your google drive and click on \"New\" and then \"File upload\". Which will open a pop-up of your local computer drive where you can naviagte to your downloads file, click on pooled.csv, and then click \"Open\". Which will upload the pooled.csv to your google drive.\n",
        "\n",
        "Once your pooled.csv is on your google drive, naviagte to the 6th code block on your copy of the project. It will look like the following code block."
      ],
      "metadata": {
        "id": "4ATjohGcI2gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# load in pooled.csv\n",
        "file = ('/content/drive/MyDrive/pooled.csv')\n",
        "pooled_df = pd.read_csv(file)\n",
        "\n",
        "# convert pooled_output column to list of numbers (currently strings)\n",
        "# will take a few minutes to run\n",
        "def string_to_nums(list_of_strings):\n",
        "    numbers_as_strings = list_of_strings.strip('[]\\n').split()\n",
        "    return [ast.literal_eval(num) for num in numbers_as_strings]\n",
        "\n",
        "pooled_df['pooled_output'] = pooled_df['pooled_output'].apply(string_to_nums)\n",
        "\n",
        "# split the dataset into training and testing sets where the test set is 20% of the original dataset.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(pooled_df['pooled_output'].tolist(), pooled_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# convert Y labels to an numpy array. Recall that the X value are already converted to numpy arrays during encode_and_pool.\n",
        "Y_train = Y_train.values\n",
        "Y_test = Y_test.values"
      ],
      "metadata": {
        "id": "3IwzUEVKax6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once there, click on the \"Files\" button on the left-most side of your screen. Then click on \"drive\" and then \"MyDrive\". Find the pooled.csv within your google-drive files and right click it. Then press on \"Copy Path\". Find the line that reads \"file = ('/content/drive/MyDrive/pooled.csv')\" within the code block. Delete the part that reads /content/drive/MyDrive/pooled.csv and copy and paste your pooled.csv pathway into its location.\n",
        "\n",
        "Go ahead and run the current code block afterwards. It will load in your pooled.csv containing 20,000 encoded and pooled samples and their sentiment from the IMDB dataset as strings.\n",
        "\n",
        "Then it will convert the strings of the pooled output to floats. Creating a training and testing set of the dataframe afterwards such that 80% of the dataframe goes towards training and 20% goes towards testing. Finally, it will convert the positive and negative sentiment labels to being stored in a numpy array to meet the conditions of the machine learning models we will use in moment from sklearn.\n",
        "\n",
        "Next, we will run the code block directly after the one we just ran. It will look like the following code block."
      ],
      "metadata": {
        "id": "qEenRbGSa9tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# hypertuning k for KNN\n",
        "\n",
        "# generate k values\n",
        "k_values = range(1, 300, 2)\n",
        "\n",
        "# set parameter to tune (k values)\n",
        "grid_params = { 'n_neighbors' : k_values}\n",
        "\n",
        "# create instance of gridsearch\n",
        "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n",
        "\n",
        "# fit the model on our train set\n",
        "g_res = gs.fit(X_train, Y_train)\n",
        "\n",
        "# find the best score\n",
        "g_res.best_score_\n",
        "\n",
        "# get the hyperparameters with the best score\n",
        "opt_k_value = g_res.best_params_['n_neighbors']\n",
        "print(opt_k_value)"
      ],
      "metadata": {
        "id": "6JR4Dq5BbPpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block will import a KNN model from sklearn for you to use later on in the program as well as the GridSearchCV function that will fit and score each of our potential k values for you. It will then generate a range of k values from 1 to 300 and then test those k values for a KNN model. Telling you what the optimal k value is and storing it in the opt_k_value variable.\n",
        "\n",
        "When we ran this code, our optimal k value was 17. Yours should be the same.\n",
        "\n",
        "After running this code block, go ahead and run the following four code blocks on your copy of the project. They will import a KNN, SVM, Perceptron, and Decision Tree classifier respectfully. As well as train each model on the training set and make predictions on the testing set using each model. Note that the KNN model will use our optimal k value during training.  \n",
        "\n",
        "Further, note that running all four code blocks on your copy of the project may take around 15 minutes, with the SVM taking the longest at around 5 minutes.\n",
        "\n",
        "Once you have ran the code blocks for all four models, go ahead and run the last code block on your copy of the project. It will look like the following code block."
      ],
      "metadata": {
        "id": "cUNhMxz0bXAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate models' performance via f1 score and bootstrapping\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Bootstrapping and F1 measurement for 10 bootstrap iterations\n",
        "KNN_scores = []\n",
        "SVM_scores = []\n",
        "Perceptron_scores = []\n",
        "DT_scores = []\n",
        "\n",
        "for i in range(10):  # for each bootstrap iteration\n",
        "  boot_samples = np.random.randint(low = 0, high = len(Y_test)-1, size = 1000) # sample (with replacement) a set of indices of 1000 test samples\n",
        "\n",
        "  # get their corresponding prediction\n",
        "  KNN_Y_pred_boot = KNN_Y_test_pred[boot_samples]\n",
        "  SVM_Y_pred_boot = SVM_Y_test_pred[boot_samples]\n",
        "  Perceptron_Y_pred_boot = Perceptron_Y_test_pred[boot_samples]\n",
        "  DT_Y_pred_boot = DT_Y_test_pred[boot_samples]\n",
        "\n",
        "  # get their corresponding ground-truth value\n",
        "  Y_test_boot = Y_test[boot_samples]\n",
        "\n",
        "  # evaluate the F1 measurement & store\n",
        "  KNN_scores.append(f1_score(Y_test_boot, KNN_Y_pred_boot, pos_label='positive'))\n",
        "  SVM_scores.append(f1_score(Y_test_boot, SVM_Y_pred_boot, pos_label='positive'))\n",
        "  Perceptron_scores.append(f1_score(Y_test_boot, Perceptron_Y_pred_boot, pos_label='positive'))\n",
        "  DT_scores.append(f1_score(Y_test_boot, DT_Y_pred_boot, pos_label='positive'))\n",
        "\n",
        "KNN_scores = np.array(KNN_scores)\n",
        "SVM_scores = np.array(SVM_scores)\n",
        "Perceptron_scores = np.array(Perceptron_scores)\n",
        "DT_scores = np.array(DT_scores)\n",
        "\n",
        "print(f\"Average F1 Score for KNN: {KNN_scores.mean()}\")\n",
        "print(f\"Average F1 Score for SVM: {SVM_scores.mean()}\")\n",
        "print(f\"Average F1 Score for Perceptron: {Perceptron_scores.mean()}\")\n",
        "print(f\"Average F1 Score for Decision Tree: {DT_scores.mean()}\")"
      ],
      "metadata": {
        "id": "S8yInxwIb8Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final code block will perform 10 bootstrapping iterations with 1,000 randomly chosen test samples. Within each bootstrapped iteration, for each model (KNN, SVM, Perceptron, and Decision Tree), it will gather the corresponding predicted prediction sentiments for each of the 1,000 randomly chosen test samples. Then it will gather the true prediction sentiments for each of the randomly chosen test samples and compare the predicted predictions with the true predictions for each model via F1 score. Recall that the F1 score is calculated as $\\frac{2(percision*recall)}{percision + recall}$. The F1 score for each model in each iteration will be saved to a list.\n",
        "\n",
        "After all 10 F1 scores are calculated for each model during bootstrapping, we convert the lists of F1 scores to numpy arrays and take the mean of each array. This mean is the average F1 score for each model after all 10 iterations of bootstrapping and it will be printed to the screen.\n",
        "\n",
        "When we ran our code we got an average F1 score for each model as follows: \\\\\n",
        "Average F1 Score for KNN: 0.7611507903070905 \\\\\n",
        "Average F1 Score for SVM: 0.8733311273376934 \\\\\n",
        "Average F1 Score for Perceptron: 0.7376342464208909 \\\\\n",
        "Average F1 Score for Decision Tree: 0.7020079642148629 \\\\\n",
        "\n",
        "Which means that our SVM out performed our KNN, Perceptron, and Decision Tree models. Your copy of the project should have a similiar average F1 score for all four models though it may vary due to randomly selected sampling aspect of bootstrapping."
      ],
      "metadata": {
        "id": "xeLInxx1cGPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Option 2</h3>\n",
        "\n",
        "First, you will need to download our pooled.csv to you google drive. In order to do this you will need to click on the following link: https://drive.google.com/file/d/1uhUXBDdwGaTIDWebfjaUJta-s1P489wI/view?usp=sharing. It will take you to our pooled.csv containing the encoded and pooled format of the IMDB dataset. From there, you can click on \"Download\" which will bring you to a page that will ask if you still want to download the file eventhough google drive can't scan it for viruses. Once there, click on \"Download anyway\" and the pooled.csv will be saved to your local computer's downloads file.\n",
        "\n",
        "Next, naviagte back to your google drive and click on \"New\" and then \"File upload\". Which will open a pop-up of your local computer drive where you can naviagte to your downloads file, click on pooled.csv, and then click \"Open\". Which will upload the pooled.csv to your google drive.\n",
        "\n",
        "From there, navigate back to your google drive and open up your copy of the project. Go ahead and run the first block of code on your copy of the project which will look like the following code block."
      ],
      "metadata": {
        "id": "H4ZBKwnyseRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connect to google drive to load in dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0zNb400mxxHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce06c7c-7bab-49d9-c6b9-b4523dc1acac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will open a pop-up that will ask you to connect your local google drive to the project. Click on \"Connect to Google Drive\" and then use the following pop-up to log into your google drive account. Once logged into your account, press \"Allow\" and your google drive will be synced to the project.\n",
        "\n",
        "Once your google drive is mounted, naviagte to the following code block on your copy of the project."
      ],
      "metadata": {
        "id": "RrDrRkF9sVhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "# load in pooled.csv\n",
        "file = ('/content/drive/MyDrive/pooled.csv')\n",
        "pooled_df = pd.read_csv(file)\n",
        "\n",
        "# convert pooled_output column to list of numbers (currently strings)\n",
        "# will take a few minutes to run\n",
        "def string_to_nums(list_of_strings):\n",
        "    numbers_as_strings = list_of_strings.strip('[]\\n').split()\n",
        "    return [ast.literal_eval(num) for num in numbers_as_strings]\n",
        "\n",
        "pooled_df['pooled_output'] = pooled_df['pooled_output'].apply(string_to_nums)\n",
        "\n",
        "# split the dataset into training and testing sets where the test set is 20% of the original dataset.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(pooled_df['pooled_output'].tolist(), pooled_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# convert Y labels to an numpy array. Recall that the X value are already converted to numpy arrays during encode_and_pool.\n",
        "Y_train = Y_train.values\n",
        "Y_test = Y_test.values"
      ],
      "metadata": {
        "id": "_mXf9c5KtQfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once there, click on the \"Files\" button on the left-most side of your screen. Then click on \"drive\" and then \"MyDrive\". Find the pooled.csv within your google-drive files and right click it. Then press on \"Copy Path\". Find the line that reads \"file = ('/content/drive/MyDrive/pooled.csv')\" within the code block. Delete the part that reads /content/drive/MyDrive/pooled.csv and copy and paste your pooled.csv pathway into its location.\n",
        "\n",
        "Go ahead and run the current code block afterwards. It will load in the pooled.csv containing 20,000 encoded and pooled samples and their sentiment from the IMDB dataset as strings. 10,000 of the samples are of positive sentiment and 10,000 of the samples are of negative sentiment.\n",
        "\n",
        "Then it will convert the strings of the pooled output to floats. Creating a training and testing set of the dataframe afterwards such that 80% of the dataframe goes towards training and 20% goes towards testing. Finally, this code block will convert the positive and negative sentiment labels to being stored in a numpy array to meet the conditions of the machine learning models we will use in moment from sklearn.\n",
        "\n",
        "Next, we will run the code block directly after the one we just ran. It will look like the following code block."
      ],
      "metadata": {
        "id": "PJT_7Paktiws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# hypertuning k for KNN\n",
        "\n",
        "# generate k values\n",
        "k_values = range(1, 300, 2)\n",
        "\n",
        "# set parameter to tune (k values)\n",
        "grid_params = { 'n_neighbors' : k_values}\n",
        "\n",
        "# create instance of gridsearch\n",
        "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n",
        "\n",
        "# fit the model on our train set\n",
        "g_res = gs.fit(X_train, Y_train)\n",
        "\n",
        "# find the best score\n",
        "g_res.best_score_\n",
        "\n",
        "# get the hyperparameters with the best score\n",
        "opt_k_value = g_res.best_params_['n_neighbors']\n",
        "print(opt_k_value)"
      ],
      "metadata": {
        "id": "ieUEkTDmykTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block will import a KNN model from sklearn for us to use later on in the program as well as the GridSearchCV function that will fit and score each of our potential k values for us. It will then generate a range of k values from 1 to 300 and then test those k values for a KNN model. Telling what the optimal k value is and storing it in the opt_k_value variable.\n",
        "\n",
        "When we ran this code, our optimal k value was 17. Yours should be the same.\n",
        "\n",
        "After running this code block, go ahead and run the following four code blocks on your copy of the project. They will import a KNN, SVM, Perceptron, and Decision Tree classifier respectfully. As well as train each model on the training set and make predictions on the testing set using each model. Note that the KNN model will use our optimal k value during training.  \n",
        "\n",
        "Further, note that running all four code blocks on your copy of the project may take around 15 minutes, with the SVM taking the longest at around 5 minutes.\n",
        "\n",
        "Once you have ran the code blocks for all four models, go ahead and run the last code block on your copy of the project. It will look like the following code block."
      ],
      "metadata": {
        "id": "bDLVEWmxysqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate models' performance via f1 score and bootstrapping\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Bootstrapping and F1 measurement for 10 bootstrap iterations\n",
        "KNN_scores = []\n",
        "SVM_scores = []\n",
        "Perceptron_scores = []\n",
        "DT_scores = []\n",
        "\n",
        "for i in range(10):  # for each bootstrap iteration\n",
        "  boot_samples = np.random.randint(low = 0, high = len(Y_test)-1, size = 1000) # sample (with replacement) a set of indices of 1000 test samples\n",
        "\n",
        "  # get their corresponding prediction\n",
        "  KNN_Y_pred_boot = KNN_Y_test_pred[boot_samples]\n",
        "  SVM_Y_pred_boot = SVM_Y_test_pred[boot_samples]\n",
        "  Perceptron_Y_pred_boot = Perceptron_Y_test_pred[boot_samples]\n",
        "  DT_Y_pred_boot = DT_Y_test_pred[boot_samples]\n",
        "\n",
        "  # get their corresponding ground-truth value\n",
        "  Y_test_boot = Y_test[boot_samples]\n",
        "\n",
        "  # evaluate the F1 measurement & store\n",
        "  KNN_scores.append(f1_score(Y_test_boot, KNN_Y_pred_boot, pos_label='positive'))\n",
        "  SVM_scores.append(f1_score(Y_test_boot, SVM_Y_pred_boot, pos_label='positive'))\n",
        "  Perceptron_scores.append(f1_score(Y_test_boot, Perceptron_Y_pred_boot, pos_label='positive'))\n",
        "  DT_scores.append(f1_score(Y_test_boot, DT_Y_pred_boot, pos_label='positive'))\n",
        "\n",
        "KNN_scores = np.array(KNN_scores)\n",
        "SVM_scores = np.array(SVM_scores)\n",
        "Perceptron_scores = np.array(Perceptron_scores)\n",
        "DT_scores = np.array(DT_scores)\n",
        "\n",
        "print(f\"Average F1 Score for KNN: {KNN_scores.mean()}\")\n",
        "print(f\"Average F1 Score for SVM: {SVM_scores.mean()}\")\n",
        "print(f\"Average F1 Score for Perceptron: {Perceptron_scores.mean()}\")\n",
        "print(f\"Average F1 Score for Decision Tree: {DT_scores.mean()}\")"
      ],
      "metadata": {
        "id": "px1PMvry4tiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final code block will perform 10 bootstrapping iterations with 1,000 randomly chosen test samples. Within each bootstrapped iteration, for each model (KNN, SVM, Perceptron, and Decision Tree), it will gather the corresponding predicted prediction sentiments for each of the 1,000 randomly chosen test samples. Then it will gather the true prediction sentiments for each of the randomly chosen test samples and compare the predicted predictions with the true predictions for each model via F1 score. Recall that the F1 score is calculated as $\\frac{2(percision*recall)}{percision + recall}$. The F1 score for each model in each iteration will be saved to a list.\n",
        "\n",
        "After all 10 F1 scores are calculated for each model during bootstrapping, we convert the lists of F1 scores to numpy arrays and take the mean of each array. This mean is the average F1 score for each model after all 10 iterations of bootstrapping and it will be printed to the screen.\n",
        "\n",
        "When we ran our code we got an average F1 score for each model as follows: \\\\\n",
        "Average F1 Score for KNN: 0.7611507903070905 \\\\\n",
        "Average F1 Score for SVM: 0.8733311273376934 \\\\\n",
        "Average F1 Score for Perceptron: 0.7376342464208909 \\\\\n",
        "Average F1 Score for Decision Tree: 0.7020079642148629 \\\\\n",
        "\n",
        "Which means that our SVM out performed our KNN, Perceptron, and Decision Tree models. Your copy of the project should have a similiar average F1 score for all four models though it may vary due to randomly selected sampling aspect of bootstrapping."
      ],
      "metadata": {
        "id": "ohZ_lqet44wM"
      }
    }
  ]
}